\documentclass{amsart}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{hyperref}
\usepackage[singlelinecheck=false]{caption}
\usepackage[backend=biber,url=true,doi=true,eprint=false,style=alphabetic]{biblatex}
\usepackage{enumitem}
\usepackage[justification=centering]{caption}
\usepackage{indentfirst}
\usepackage{algorithm}
\usepackage{algpseudocode}

\addbibresource{references.bib}

\makeatletter
\def\subsection{\@startsection{subsection}{3}%
  \z@{.5\linespacing\@plus.7\linespacing}{.1\linespacing}%
  {\normalfont\itshape}}
\makeatother

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand\defeq{\mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny\sffamily def}}}{=}}}

\algrenewcommand\algorithmicrequire{\textbf{Input}}
\algrenewcommand\algorithmicensure{\textbf{Output}}

\captionsetup[table]{labelsep=space}

\theoremstyle{plain}

\newtheorem*{definition}{Definition}
\newtheorem*{theorem}{Theorem}
\newtheorem{proposition}{Proposition}

\newcommand{\set}[1]{\mathcal{#1}}
\newcommand{\pr}{\mathbb{P}}
\renewcommand{\implies}{\Rightarrow}

\setlength{\parskip}{1em}

\title[]{2. Bayesian Networks}
\author[]{Renato Lui Geh\\NUSP\@: 8536030}

\begin{document}

\begin{abstract}
  In this document we present general, basic notions of bayesian networks. We show how to 
  \vspace*{-2.5em}
\end{abstract}

\maketitle

\section{Reasoning and Uncertainty in Artificial Intelligence}

Before the advent of probability theory on uncertainty, it was believed that it was only possible
to represent knowledge through the use of logic and other neo-calculist techniques. Although logic
does have its praises, such as its computational ease and natural semantics that resemble human
reasoning, it is problematic when one tackles the issue of uncertainty. When it comes to
representing the world, propositional and first order logic must assume a binary reading of data:
true or false. Indeed the world is not so simple, and this vision is, at best, simplistic, since we
find that degrees of beliefs are present everywhere. For example, one can not say that all birds
fly, for it may have its wing broken. Of course we could enumerate all instances where the given
bird cannot fly (e.g.\ broken wing, bird species, etc), but this approach is exhausting and
unrealistic. This issue can be ameliorated (if not solved) by the introduction of probabilities as
a means to measure degrees of belief. We could then say that the bird has a 90\% chance of flying,
discarding the remaining 10\% as a sum of all probabilities of an infinite set of improbable events
that could prevent the bird from flying without having to enumerate each one of them. However the
introduction of probability as a direct method of computing uncertainty was viewed with skepticism
among AI researchers. Although not so welcomed at first, probability theory gained incredible
support in the last decades, with influential researchers such as Judea Pearl actively supporting
its use~\cite{pearl-1988}. In this report we first take a look at the formalisms of propositional
logic when representing knowledge, analysing the semantics, properties and relationships. Once
we are familiarized with the difficulties logic presents to uncertainty we introduce the basics
of probability theory and how to compute simple queries.

\section{Propositional Logic}

Be it through probabilities to represent degrees of belief or through Boolean values to represent
truths of events, one needs to define a language to describe knowledge formally. When it comes to
representing truths, an appropriate language is propositional logic. The atomic element in
propositional logic is the variable. A variable $X$ takes values in a domain $\Omega$. Such value
assignment is represented as the sentence $X=x$, for $x\in \Omega$. A sentence is defined as
follows.

\begin{itemize}
  \item A variable $X$ is a sentence.
  \item If $\alpha$ and $\beta$ are sentences, then $\alpha \wedge \beta$, $\alpha \vee \beta$ and
    $\neg \alpha$ are sentences.
\end{itemize}

The sentence $\alpha \wedge \beta$ represents disjointion, $\alpha \vee \beta$ is called
conjunction and $\neg \alpha$ negation. The symbols $\wedge$, $\vee$ and $\neg$ are called logical
connectives and are the main constructs of propositional logic. Valuations to a variable $X$ to a
value $x\in \Omega$ are described via a function $\nu$. We say that $\nu \models \phi$ if $\nu$
satisfies the sentence $\phi$. That is, the valuation of $\phi$ belongs to the domain $\Omega$ that
$\nu$ maps. If $\nu$ does not satisfy $\phi$, we say that $\nu \not\models \phi$. This means $\phi$
is not in the domain and therefore $\nu \models \neg \phi$. For conjunction, we say that $\nu
\models \phi \vee \psi \iff \nu \models \phi$ or $\nu \models \psi$. Similarly, $\nu \models \phi
\wedge \psi \iff \nu \models \phi$ and $\nu \models \psi$. By guaranteeing that negation,
conjunction and disjointion obey these rules, one defines a field of sets. A field of sets is a
pair $(\Omega,\mathcal{F})$ where $\Omega$ is the set of valuations and $\mathcal{F}$ is the set
of possible outcomes closed under negation, union (i.e.\ conjunction) and intersection (i.e.\
disjointion).

\begin{proposition} (Exercise 7)\\
  Let $\Omega$ be the set of all valuations and $\mathcal{F}$ be the set of events.
  \begin{equation*}
    \alpha \coloneqq \{\nu \in \Omega : \nu \models \phi\}\text{, when $\phi$ is a sentence.}
  \end{equation*}
  Then $(\Omega,\mathcal{F})$ is a field of sets.
\end{proposition}

\begin{proof}
  To prove $(\Omega,\mathcal{F})$ is a field of sets we must show that $\mathcal{F}$ is a non-empty
  set closed under union, intersection and complement. Let $\phi,\psi \in \Omega$, $\nu \models
  (\phi \vee \psi) \iff \nu \models \phi$ or $\nu \models \psi$. But from our earlier assumption,
  $\phi,\psi \in \Omega$; therefore $\mathcal{F}$ is closed under union. Similarly, $\nu \models
  (\phi \wedge \psi) \iff \nu \models \phi$ and $\nu \models \psi$. Since both sentences $\phi,\psi
  \in \Omega$ then $\mathcal{F}$ is closed under intersection. Finally, for an arbitrary $\phi$,
  $\nu \models \neg \phi \iff \nu \not\models \phi$. Assume $\phi \in \Omega$. Then $\phi$
  satisfies in $\Omega$ and therefore $\neg\phi$ does not satisfy lest we create a contradiction.
  Thus, by the definition of satisfiabilitiy $\nu \models \neg\phi$. Now consider $\nu \not\models
  \neg\phi$, then $\nu \models \neg(\neg\phi)$ and so $\neg\phi$ is in $\Omega$. Therefore,
  $\mathcal{F}$ is closed under complements.
\end{proof}

\section{Probability Calculus}

Whilst in propositional logic we had a binary set of possible values to variables, in probability
theory we have a contiguous interval of possible valuations. These possible values represent a
certain degree of belief that an agent may consider for an event. That is, for an event $\alpha$
to occur in the set of possible outcomes $\omega$, we may define the degree of belief, or
probability, as a function $\pr$ such that:

\begin{equation*}
  \pr(\alpha) \defeq \sum_{\omega\models\alpha} \pr(\omega)
\end{equation*}

In other words, the probability of event $\alpha$ to occur is the summation of every worlds $w_i$
in which $\alpha$ is satisfiable. Of course one must guarantee that $\pr$ follows the axioms of
probability.

\begin{definition}
  A function $\pr$ is a probability function if, for every $\alpha \in \Omega$ where $\Omega$ is
  the set of valuations in the field of sets $(\Omega,\mathcal{F})$, $\pr$ obeys the following
  probability axioms:

  \begin{enumerate}
    \item $\pr(\alpha) \in \mathbb{R}, \pr(\alpha) \leq 0$
    \item $\pr(\Omega)=1$
    \item $\pr(\bigcup_i^n \alpha_i) = \sum_i^n \pr(\alpha)$ if $\bigcap_i^n \alpha_i=\emptyset$,
      that is, $\alpha_i$ are disjoint sets.
  \end{enumerate}
\end{definition}

From these axioms we derive the following properties.

\begin{proposition} (Exercise 1.1)\\
  For any event $\alpha$, it follows that $\pr(\alpha)=1-\pr(\alpha^c)$, where $\alpha^c$ is
  the complement of $\alpha$.
\end{proposition}

\begin{proof}
  Let $\Omega$ be the space of outcomes in which $\alpha$ is contained in. Since $\alpha$ and
  $\alpha^c$ are disjoint and exhaustive then from the axiom $\pr(\Omega)=1$, we have that
  $\pr(\alpha)+\pr(\alpha^c)=\pr(\Omega)=1$. From there we have $\pr(\alpha)=1-\pr(\alpha^c)$.
\end{proof}

\begin{proposition} (Exercise 1.2)\\
  $\pr(\emptyset)=0$
\end{proposition}

\begin{proof}
  Assume that $\pr(\emptyset)>0$. From the probability axioms we have that $\pr(\alpha\cup\beta)=
  \pr(\alpha)+\pr(\beta)$ if $\alpha$ and $\beta$ are disjoint. Then $\pr(\emptyset\cup\emptyset)=
  \pr(\emptyset)+\pr(\emptyset)$, since an empty set is disjoint with itself. But $\emptyset\cup
  \emptyset=\emptyset$, therefore $\pr(\emptyset)=2\pr(\emptyset)$. Which is a contradiction if
  $\pr(\emptyset)>0$. Thus $\pr(\emptyset)=0$.
\end{proof}

\begin{proposition} (Exercise 1.3)\\
  If $\alpha\subseteq\beta$ are events then $\pr(\alpha)\leq\pr(\beta)$. This is called
  monotonicity.
\end{proposition}

\begin{proof}
  Assume that $\pr(\alpha)>\pr(\beta)$ when $\alpha\subseteq\beta$. Now consider $\alpha=
  \emptyset$: $\pr(\emptyset)>\pr(\beta)$. But $\pr(\emptyset)=0\Rightarrow0>\pr(\beta)$, and from
  the probability axiom, $\pr(\beta)\geq0$ which is a contradiction. Therefore $\pr(\alpha)\geq
  \pr(\beta)$.
\end{proof}

\begin{proposition} (Exercise 1.4)\\
  For every event $\alpha$, we have that $0\leq\pr(\alpha)\leq1$.
\end{proposition}

\begin{proof}
  From the first probability axiom we have that $\pr(\alpha)\geq0$. Consider $\pr(\alpha)>1$. From
  $\pr(\alpha)=1-\pr(\alpha^c)$ we have that $\pr(\alpha^c)<0$ which contradicts the first
  probability axiom. Thus $0\leq\pr(\alpha)\leq1$.
\end{proof}

\begin{proposition} (Exercise 1.5)\\
  For any events $\alpha$ and $\beta$ (not necessarily disjoint), we have that $\pr(\alpha\cup
  \beta)=\pr(\alpha)+\pr(\beta)+\pr(\alpha\cap\beta)$.
\end{proposition}

\begin{proof}
  From the third axiom, $\pr(\beta)=\pr(\alpha\cap\beta)+\pr(\beta\setminus(\alpha\cap\beta))$,
  where $\alpha\cap\beta$ and $\beta\setminus(\alpha\cap\beta)$ are disjoint. The third axiom of
  probability, $\pr(\bigcup_i X_i)=\sum_i\pr(X_i)$, is for disjoint sets. Therefore: $\pr(\alpha
  \cup(\beta\setminus(\alpha\cap\beta)))=\pr(\alpha)+\pr(\beta\setminus(\alpha\cap\beta))$.
  Substituting $\pr(\beta\setminus(\alpha\cap\beta))$ with $\pr(\beta)-\pr(\alpha\cap\beta)$, $\pr(
  \alpha\cup(\beta\setminus(\alpha\cap\beta)))=\pr(\alpha)+\pr(\beta)-\pr(\alpha\cap\beta)$. But
  $\alpha\cup(\beta\setminus(\alpha\cap\beta))=\alpha\cup\beta$, resulting in $\pr(\alpha\cup\beta)
  =\pr(\alpha)+\pr(\beta)-\pr(\alpha\cap\beta)$.
\end{proof}

\begin{proposition} (Exercise 1.6)\\
  If $\pr(\alpha)=1$ for some event $\alpha$ then $\pr(\beta)=\pr(\alpha\cap\beta)$ for any
  event $\beta$.
\end{proposition}

\begin{proof}
  $\pr(\alpha\cup\beta)=\pr(\alpha)+\pr(\beta)-\pr(\alpha\cap\beta)$. Since $\pr(\alpha)=1$ and
  if $\alpha$ happens with certainty then $\pr(\alpha\cup\beta)=\pr(\Omega)=1$, then $1=1+
  \pr(\beta)-\pr(\alpha\cap\beta)\Rightarrow\pr(\beta)=\pr(\alpha\cap\beta)$.
\end{proof}

\subsection{Conditional probability}

Belief updating in probability is done through conditional probabilities. Intuitively, one could
assume belief updating as a change in belief given new evidence. For example, we could assume that
the probability of a train arriving on time to our station is $\pr(OnTime=true)=0.3$. But say we
receive news from our neighboring station that the train has arrived on time to their station. We
could safely assume that this fact increases our belief that the train will arrive on time, since
we received news it has not yet delayed. That is, given the evidence that the train has not delayed
on the $i$-th station, our probability of our train arriving on time is $\pr(OnTime=true|
StationDelayed_i=false)=0.8$. A probability function of the form $\pr(X|E)$ is called a conditional
or posterior probability. The variable $X$ is called the query variable and $E$ the evidence.

\begin{definition}
  A conditional probability $\pr(\alpha|\beta)$ is defined as:
  \begin{equation*}
    \pr(\alpha|\beta)=\frac{\pr(\alpha\cap\beta)}{\pr(\beta)}, \pr(\beta)>0
  \end{equation*}
\end{definition}

Just as in probabilities of the form $\pr(X)$, which are called prior probabilities, conditional
probabilities obey the three probability axioms:

\begin{enumerate}
  \item $\pr(\alpha|\beta)\geq0$
  \item $\pr(\Omega|\beta)=1$
  \item $\pr(\alpha\cup\beta|\gamma)=\pr(\alpha|\gamma)+\pr(\beta|\gamma)$ for $\alpha$ and $\beta$
    disjoint.
\end{enumerate}

\begin{proposition} (Exercise 2)\\
  Let $\pr$ be a probability function (or some probability space). Show that for any event $\beta$
  with $\pr(\beta)>0$ the function $\pr'(\alpha)=\pr(\alpha|\beta)$ is also a probability fuction.
\end{proposition}

\begin{proof}
  From the definition of conditional probability:

  \begin{equation*}
    \pr(\alpha|\beta)=\frac{\pr(\alpha\cap\beta)}{\pr(\beta)}
  \end{equation*}

  Since $\pr(\beta)>0$, $\pr(\alpha\cap\beta)/\pr(\beta)$ is defined for all possible values and
  $\pr(\alpha|\beta)\geq0$ (first axiom), because $\pr(\alpha\cap\beta)\geq0$ and $\pr(\beta)>0$.

  \begin{equation*}
    \pr(\alpha|\beta)=\frac{\pr(\alpha)+\pr(\beta)-\pr(\alpha\cup\beta)}{\pr(\beta)}\leq1
  \end{equation*}

  Because $\pr(\alpha)+\pr(\beta)-\pr(\alpha\cup\beta)\leq\pr(\beta)\Rightarrow\pr(\alpha)\leq\pr(
  \alpha\cup\beta)$ which is true since if $X\subseteq Y$ then $\pr(X)\leq\pr(Y)$. Substituting $X$
  for $\alpha$ and $Y$ for $\alpha\cup\beta$ we have that $\pr(\alpha)\leq\pr(\alpha\cup\beta)$.
  Thus $\pr(\alpha|\beta)\leq1$ (second axiom). For the third axiom suppose $\alpha$ and $\gamma$
  are disjoint.

  \begin{equation*}
    \pr(\alpha\cup\gamma|\beta)=\frac{\pr((\alpha\cup\gamma)\cap\beta)}{\pr(\beta)}=\frac{\pr((
      \alpha\cap\beta)\cup(\gamma\cap\beta))}{\pr(\beta)}
  \end{equation*}

  $\alpha\cap\beta$ and $\gamma\cap\beta$ are disjoint since $\alpha$ and $\gamma$ are disjoint.
  Therefore we may apply axiom three:

  \begin{equation*}
    \pr(\alpha\cap\gamma|\beta)=\frac{\pr(\alpha\cap\beta)+\pr(\gamma\cap\beta)}{\pr(\beta)}=
    \frac{\pr(\alpha\cap\beta)}{\pr(\beta)}=\frac{\pr(\gamma\cap\beta)}{\pr(\beta)}=\pr(\alpha|
    \beta)+\pr(\gamma|\beta)
  \end{equation*}
\end{proof}

\subsection{Chain Rule}

\begin{proposition} (Exercise 3)\\
  For any sequence of events $\alpha_1,\ldots,\alpha_n$ we have that:
  \begin{equation*}
    \pr(\alpha_1\cap\ldots\cap\alpha_n)=\pr(\alpha_1)\prod_{i=2}^n\pr(\alpha_i|\alpha_1\cap\ldots
    \cap\alpha_{i-1}
  \end{equation*}
\end{proposition}

\begin{proof}
  We will prove by induction. For the base $n=2$ we have, directly from the definition of
  conditional probability:

  \begin{equation*}
    \pr(\alpha_2|\alpha_1)=\frac{\pr(\alpha_1\cap\alpha_2)}{\pr(\alpha_1)}\Rightarrow\pr(\alpha_1
    \cap\alpha_2)=\pr(\alpha_1)\pr(\alpha_2|\alpha_1)
  \end{equation*}

  For the $k$-th case:

  \begin{equation*}
    \pr(\alpha_k|\alpha_1\cap\ldots\cap\alpha_{k-1})=\frac{\pr(\alpha_1\cap\ldots\cap\alpha_k)}
    {\pr(\alpha_1\cap\ldots\cap\alpha_{k-1})}
  \end{equation*}

  \begin{equation*}
    \text{(i) }\pr(\alpha_1\cap\ldots\cap\alpha_k)=\pr(\alpha_1\cap\ldots\cap\alpha_{k-1})
      \pr(\alpha_k|\alpha_1\cap\ldots\cap\alpha_{k-1})
  \end{equation*}

  But $\pr(\alpha_1\cap\ldots\cap\alpha_{k-1})$ can be transformed into

  \begin{equation*}
    \text{(ii) }\pr(\alpha_1\cap\ldots\cap\alpha_{k-1})=\pr(\alpha_1\cap\ldots\cap\alpha_{k-2})
      \pr(\alpha_{k-1}|\alpha_1\cap\ldots\cap\alpha_{k-2})
  \end{equation*}

  Applying (ii) in (i) we have:

  \begin{equation*}
    \pr(\alpha_1\cap\ldots\cap\alpha_k)=\pr(\alpha_1\cap\ldots\cap\alpha_{k-2})\pr(\alpha_{k-1}|
      \alpha_1\cap\ldots\cap\alpha_{k-2})\pr(\alpha_k|\alpha_1\cap\ldots\cap\alpha_{k-1})
  \end{equation*}

  For the $(k+1)$-th case:

  \begin{equation*}
    \pr(\alpha_{k+1}|\alpha_1\cap\ldots\cap\alpha_k)=\frac{\pr(\alpha_1\cap\ldots\cap\alpha_{k+1})}
      {\pr(\alpha_1\cap\ldots\cap\alpha_k)}
  \end{equation*}

  \begin{equation*}
    \pr(\alpha_1\cap\ldots\cap\alpha_{k+1})=\pr(\alpha_1\cap\ldots\cap\alpha_k)\pr(\alpha_{k+1}|
      \alpha_1\cap\ldots\cap\alpha_k)
  \end{equation*}

  But by induction hypothesis $\pr(\alpha_1\cap\ldots\cap\alpha_k)$ is true. Therefore for $n\geq2$
  we have:

  \begin{align*}
    \pr(\alpha_1\cap\ldots\cap\alpha_n)&=\pr(\alpha_1)\pr(\alpha_2|\alpha_1)\ldots\pr(\alpha_n|
      \alpha_1\cap\ldots\cap\alpha_{n-1})\\
                                       &=\pr(\alpha_1)\prod_{i=2}^n\pr(\alpha_i|\alpha_1\cap\ldots
      \cap\alpha_{i-1})
  \end{align*}
\end{proof}

\subsection{Total Probability Rule}

\begin{proposition} (Exercise 4)\\
  Let $\alpha_1,\ldots,\alpha_n$ be a partition of $\Omega$ (i.e.\ $\bigcup_i\alpha_i=\Omega$ and
  $\alpha_i\cap\alpha_j=\emptyset$ for $i\neq j$, $\alpha_i\neq \emptyset$). Then for any event
  $\beta$ it follows that:

  \begin{equation*}
    \pr(\beta)=\sum_{i=1}^n\pr(\beta|\alpha_i)\pr(\alpha_i)
  \end{equation*}
\end{proposition}

\begin{proof}
  Since $\alpha_1,\ldots,\alpha_n$ are disjoint, then:

  \begin{equation*}
    \text{(i) }\pr(\beta\cap(\bigcup_i\alpha_i))=\pr((\beta\cap\alpha_1)\cup\ldots\cup(\beta\cap
      \alpha_n))
  \end{equation*}

  From the definition of conditional probability, for an arbitrary $i$ we have:

  \begin{equation*}
    \text{(ii) }\pr(\beta|\alpha_i)=\frac{\pr(\beta\cap\alpha_i)}{\pr(\alpha_i)}\Rightarrow
      \pr(\beta\cap\alpha_i)=\pr(\beta|\alpha_i)\pr(\alpha_i)
  \end{equation*}

  Applying (ii) in (i) we have:

  \begin{equation*}
    \pr(\beta\cap(\bigcup_i\alpha_i))=\pr(\beta|\alpha_1)\pr(\alpha_1)+\cdots+\pr(\beta|\alpha_n)
      \pr(\alpha_n)
  \end{equation*}

  Since $\alpha_1,\ldots,\alpha_n$ are disjoint and exhaustive in $\Omega$, then $\beta\cap
  (\alpha_1\cup\ldots\cup\alpha_n)=\beta\cap\Omega=\beta$. Therefore:

  \begin{equation*}
    \pr(\beta)=\sum_i\pr(\beta|\alpha_i)\pr(\alpha_i)
  \end{equation*}
\end{proof}

\subsection{Baye's Rule}

Sometimes computing the posterior probability through the joint distribution is expensive if not
impossible. A solution to this problem is Baye's Rule.

\begin{equation*}
  \pr(\alpha|\beta)=\frac{\pr(\beta|\alpha)\pr(\alpha)}{\pr(\beta)}
\end{equation*}

This expression facilitates finding $\pr(\alpha|\beta)$ when one already has $\pr(\beta|\alpha)$
computed. A typical example to this is on medical applications. Consider a disease $\alpha$ and
symptoms $\beta$. The diagnosis of a disease given the symptoms ($\pr(\alpha|\beta)$) the patient
may be experiencing can be easily computed when one knows the symptoms the disease causes
$\pr(\beta|\alpha)$. Baye's rule can be seen as the update of a probability after one computes the
``future probability'' one would get.

\begin{proposition} (Exercise 5)\\
  Extended Baye's Rule. For any events $\alpha,\beta$ and $\gamma$ with positive probability, it
  follows that:

  \begin{equation*}
    \pr(\beta|\alpha\cap\gamma)=\frac{\pr(\alpha|\beta\cap\gamma)\pr(\beta|\gamma)}{\pr(\alpha|
    \gamma)}
  \end{equation*}
\end{proposition}

\begin{proof}
  Applying the definition of conditional probability on $\pr(\alpha\cap\beta\cap\gamma)$ we have:

  \begin{equation*}
    \pr(\beta|\alpha\cap\gamma)=\frac{\pr(\alpha\cap\beta\cap\gamma)}{\pr(\alpha\cap\gamma)}=
    \frac{\pr(\alpha\cap\beta\cap\gamma)}{\pr(\alpha|\gamma)\pr(\gamma)}
  \end{equation*}

  After we decompose $\pr(\alpha\cap\gamma)$ we now once again apply conditional probability:

  \begin{equation*}
    \pr(\beta|\alpha\cap\gamma)=\frac{\pr(\alpha\cap\beta\cap\gamma)}{\pr(\alpha|\gamma)
    \pr(\gamma)}=\frac{\pr(\alpha|\beta\cap\gamma)\pr(\beta\cap\gamma)}{\pr(\alpha|\gamma)
    \pr(\gamma)}
  \end{equation*}

  Once again we decompose $\pr(\beta\cap\gamma)$:

  \begin{equation*}
    \pr(\beta|\alpha\cap\gamma)=\frac{\pr(\alpha|\beta\cap\gamma)\pr(\beta\cap\gamma)}{\pr(\alpha|
    \gamma)\pr(\gamma)}=\frac{\pr(\alpha|\beta\cap\gamma)\pr(\beta|\gamma)\pr(\gamma)}{\pr(\alpha|
    \gamma)\pr(\gamma)}
  \end{equation*}

  Since we assume $\pr(\alpha),\pr(\beta),\pr(\gamma)>0$, then:

  \begin{equation*}
    \pr(\beta|\alpha\cap\gamma)=\frac{\pr(\alpha|\beta\cap\gamma)\pr(\beta|\gamma)}{\pr(\alpha|
    \gamma)}
  \end{equation*}
\end{proof}

\subsection{Independence}

Two events are considered independent of each other when the outcome of the first does not impact
the outcome of the second. An example to this is the toss of two coins. Ideally, the first toss
should not change the probability of the second. However, when it comes to modelling the real world
one finds that true independency between events is unrealistic. Be that as it may, it is still
possible for an event to gain independency in relation to a second event when faced with a third.
This is called conditional independence. When two variables $\alpha$ and $\beta$ are not
independent when it comes to their prior probabilities but gain independency once they are
conditioned on an another variable $\gamma$, they are said to be conditionally independent on
$\gamma$.

\begin{definition}
  Two variables $\alpha$ and $\beta$ are independent, denoted by $\alpha \perp \beta$, iff
  \begin{equation*}
    \pr(\alpha\cap\beta)=\pr(\alpha)\pr(\beta)
  \end{equation*}
\end{definition}

\begin{proposition} (Exercise 6)\\
  Show that for any two events $\alpha$ and $\beta$ with positive probabilities the following
  assertions are equivalent:
  \begin{enumerate}
    \item $\alpha$ and $\beta$ are independent
    \item $\pr(\alpha|\beta=\pr(\beta)$
    \item $\pr(\beta|\alpha)=\pr(\alpha)$
  \end{enumerate}
\end{proposition}

\begin{proof}
  If events $\alpha$ and $\beta$ are independent then, from its definition $\pr(\alpha\cap\beta)=
  \pr(\alpha)\pr(\beta)$. From the conditional probability definition:

  \begin{equation*}
    \pr(\alpha|\beta)=\frac{\pr(\alpha\cap\beta)}{\pr(\beta)}\Rightarrow\pr(\alpha\cap\beta)=
    \pr(\alpha|\beta)\pr(\beta)=\pr(\beta|\alpha)\pr(\alpha)
  \end{equation*}

  Applying the definition we have that:

  \begin{align*}
    &\pr(\alpha|\beta)\pr(\beta)=\pr(\alpha)\pr(\beta)\Rightarrow\pr(\alpha|\beta)=\pr(\alpha)
    \text{\ and}\\
    &\pr(\beta|\alpha)\pr(\alpha)=\pr(\alpha)\pr(\beta)\Rightarrow\pr(\beta|\alpha)=\pr(\beta)
  \end{align*}
\end{proof}

\newpage

\printbibliography[]

\end{document}
